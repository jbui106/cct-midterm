# 107-CCT-Midterm

Cultural Consensus Theory with PyMC by Jeslyn Bui

From my understanding Cultural Consensus Theory (CCT) provides a model that allows us to understand and evaluate at least two main things by collecting the responses of informants on a certain topic, one being the group or **cultural consensus** and the other, **informant competence**. CCT theory overall, thus depicts those who are more knowledgeable will tend to agree or answer similarly to each other. Therefore according to the cultural consensus model equation I have represented in my code, these two variables are denoted as Zj (consensus) and Di (competence). When deciding the prior distribution of Di assuming that there is no understanding of an informant's competence I choose to use a uniform distribution with the boundaries of 0.5 and 1. This is to apply the probability that an informant's competence level can fall anywhere equally between at least the equivalent of random chance to perfect competence. Following the same assumption for the consensus answer and per the assignment prompt, the prior distribution I used for Zj was a Bernoulli distribution with probability equal to 0.5. 

Moving on, my implementation of the model overall in my cct python file has four main functions. The first loads the plant knowledge data from the plant_knowledge csv file, the second implements the model given the previously mentioned priors and given equation pij = Zj × Di + (1 − Zj) × (1 − Di). The third defines the analysis of the informant competence, consensus answers, naive aggregation, and creates the visualizes the posteriors, while the last function draws samples and applies the previous three functions as necessary. Looking at the results I got after running my model code for the **competence estimates** it revealed that the most competent was informant 6 (Di = 0.871) and the least competent was informant 3 (Di = 0.561). Meaning that informant 6 was the individual who most often answered correctly or in consensus with others and the opposite for informant 3. Whereas with the **most likely consensus answer (Z)** for the 20  questions given the results was: 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1 in contrast to the **majority vote answer key** from the raw data was: 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1. Comparing the most likely consensus answers and raw data majority vote answers, 15 out of 20 have the same collective answer. Partial reasoning for the differences between the two may be because for the *majority vote answer* questions which in some cases had ties of 5 to 5 out of 10, were considered 0, it likely resulted in more 0s overall. Moreover with the *most likely consensus answer* it weighs the competence of the informants and more data which likely influenced more 1s overall. As for overall the convergence of the 4 chains taken, it can be concluded by looking at the r_hat values from the convergence diagnostic. Given that the r_hat values are all 1.0 that means that there was good convergence. 

*I have used Gemini and Claude to aid with creating my python code for cct.py in order to meet project requirements. Additionally, I have added another folder called plots to store the visulizations for the posterior competence and posterior consensus.*
