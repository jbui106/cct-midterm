# 107-CCT-Midterm

Cultural Consensus Theory with PyMC by Jeslyn Bui

From my understanding Cultural Consensus Theory (CCT) provides a model that allows us to understand and evaluate at least two main things by collecting the responses of informants on a certain topic, one being the group or **cultural consensus** and the other, **informant competence**. CCT theory overall, thus depicts those who are more knowledgeable will tend to agree or answer similarly to each other. Therefore, according to the cultural consensus model equation I have represented in my code, these two variables are denoted as Zj (consensus) and Di (competence). When deciding the prior distribution of Di assuming that there is no understanding of an informant's competence I choose to use a uniform distribution with the boundaries of 0.5 and 1. This is to apply the probability that an informant's competence level can fall anywhere equally between at least the equivalent of random chance to perfect competence. Following simiarily for the consensus answer and per the assignment prompt, the prior distribution I used for Zj was a Bernoulli distribution with probability equal to 0.5.

Moving on, my implementation of the model overall in my cct python file has four main functions. The first loads the plant knowledge data from the plant_knowledge csv file, the second implements the model given the previously mentioned priors and given equation pij = Zj × Di + (1 − Zj) × (1 − Di). The third defines the analysis of the informant competence, consensus answers, and creates the visualizations of the posteriors, while the last function draws the samples and applies the previous three functions as necessary. Looking at the results I got after running my model code for the **competence estimates** it revealed that the most competent was informant 6 (Di = ~0.871) and the least competent was informant 3 (Di = ~0.561). Meaning that informant 6 was the individual who most often answered correctly or in consensus with others and the opposite for informant 3. Whereas with the **consensus answer key** from my CCT model for the 20 questions the results were: 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1 in contrast the **majority vote answers** from the raw data were: 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1. Comparing the consensus answer key and majority vote answers, 15 out of 20 have the same collective answer. Reasoning for the few differences between the two could be because for the *majority vote answers*, in some cases where questions had ties of 5 to 5 out of 10, the answer was considered 0, resulting in more 0s overall. Moreover with the *consensus answer key*, it weighs in the competence of the informants, which I believe influenced more 1s overall. As for the model convergence of the 4 chains taken, it can be concluded by looking at the r_hat values from the convergence diagnostic. Given that the r_hat values are all 1.0 that means that there was good convergence.

*I have used Gemini and Claude to assist with creating my python code for cct.py in order to meet project requirements. Additionally, I have added another folder called plots to store the visualizations for the posterior competence and posterior consensus.*
