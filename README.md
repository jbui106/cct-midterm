# 107-CCT-Midterm

Cultural Consensus Theory with PyMC by Jeslyn Bui

From my understanding Cultural Consensus Theory provides a model that allows us to understand and evaluate at least two main things by collecting the responses of informants on a certain topic. One is group or **cultural consensus** and the other is **individual competence**. CCT theory overall thus depicts that those who are more knowledgeable will tend to agree or answer similarly to each other. Therefore according to the cultural consensus model equation I have represented in my code these two variables are denoted as Zj (consensus) and Di (competence). When deciding the prior distribution of Di assuming that there is no understanding of an informant's competence I choose to use a uniform distribution with the boundaries of 0.5 and 1. This is to equally apply the probability that an informant's competence level can fall anywhere equally between at least the equivalent of random chance to perfect competence. Following the same assumption for the consensus answer and per the assignment prompt, the prior distribution I used for Zj was a Bernoulli distribution with probability equal to 0.5. 

Moving forward my implementation of the model overall in my cct python file has four main functions. The first loads the plant knowledge data from the plant_knowledge csv file, the second implements the model given the previously mentioned priors and given equation pij = Zj × Di + (1 − Zj) × (1 − Di). The third defines the analysis of the result, informant competence, consensus answers, and naive aggregation while the last function draws samples and executes the previous three as necessary. Looking at the results I got after running my model code for the **competence estimates** it revealed that the most competent individual was informant 6 (Di = 0.871) and the least competent was informant 3 (Di = 0.561). With the **most likely consensus answer (Z)** for the 20 given questions being: 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1 whereas the **majority vote answer key** from the raw data being: 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1. Considering the most likely consensus answer and raw data majority vote, 15 out of 20 have the same collective answer. However partial reasoning for the differences between the two may be because for the *majority vote answer* questions which had ties of 5 to 5 out of 10 they were considered 0, it likely resulted in more 0s overall. Moreover with the *most likely consensus answer* it weighs the competence of the informants and more data likely influenced more 1s overall. 
As for the convergence of the 4 chains taken, it can be concluded by looking at the r_hat values from the convergence diagnostic. Given that they are all 1.0 that means that there was generally good convergence. 

*I have used Gemini and Claude to aid with creating my python code for cct.py in order to meet project requirements.*
